
# ðŸ¤– Gesture-Controlled Robotic Arm â€“ Group 4

This project demonstrates a real-time gesture-controlled robotic system using the **Yahboom DOFBOT**, **Raspberry Pi 5**, **OpenCV**, **ROS**, and a **Support Vector Machine (SVM)** classifier. Hand gestures (1, 2, or 3 fingers) are detected by a camera and used to trigger robotic movements such as pick, wave, or reset.

---

## ðŸ“Œ Features

- Real-time hand gesture recognition using OpenCV
- Lightweight SVM model trained on custom hand image dataset
- ROS-based architecture for modular communication
- DOFBOT arm controlled using Yahboom Arm_Lib library
- Fully implemented on Raspberry Pi 5

---

## ðŸ§© Hardware Used

- âœ… Yahboom DOFBOT 6-DOF robotic arm  
- âœ… Raspberry Pi 5 (with Raspberry Pi OS 64-bit)  
- âœ… USB or CSI camera (110Â° FoV recommended)  
- âœ… Power supply and servo expansion board  

---

## ðŸ’» Software Stack

- Python 3  
- OpenCV  
- scikit-learn (SVM classifier)  
- ROS (Robot Operating System â€“ e.g., ROS Noetic)  
- Arm_Lib for robotic control  

---

## ðŸ›  How It Works

1. The camera captures live video and isolates the hand region.
2. The pre-trained SVM model classifies the hand gesture (1, 2, or 3).
3. The result is published on the `/gesture_id` ROS topic.
4. The robot control node listens to this topic and triggers the appropriate robot motion.

---

## ðŸ“‚ Project Structure

```
GestureControlledArm_Group4/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ model.pkl               # Trained SVM gesture classifier
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ gesture_detect.py       # ROS node: gesture recognition and publishing
â”‚   â””â”€â”€ robot_behavior.py       # ROS node: robot behavior subscriber
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ one/                    # Training images for 1-finger gesture
â”‚   â”œâ”€â”€ two/                    # Training images for 2-finger gesture
â”‚   â””â”€â”€ three/                  # Training images for 3-finger gesture
â”œâ”€â”€ gesture_train.py            # SVM training script
â”œâ”€â”€ launch/
â”‚   â””â”€â”€ gesture_control.launch  # Launch file for both nodes
â””â”€â”€ README.md                   # This file
```

---

## ðŸš€ Setup & Execution

### 1. Install Dependencies

```bash
sudo apt update
sudo apt install python3-opencv python3-pip
pip3 install scikit-learn joblib
```

Make sure ROS is installed and sourced, and `Arm_Lib` is set up.

---

### 2. Train Model (Optional)

If you want to retrain the model:

```bash
python3 gesture_train.py
```

---

### 3. Run the Project

In one terminal:

```bash
roscore
```

In another terminal:

```bash
roslaunch dofbot_gesture_control gesture_control.launch
```

---

## âœ‹ Gesture Mapping

| Gesture | ID | Robot Action       |
|---------|----|--------------------|
| 1 Finger | 1 | Pick Object        |
| 2 Fingers | 2 | Wave Motion        |
| 3 Fingers | 3 | Reset Position     |

---



## ðŸ§ª Team Members â€“ Group 4

Mahya
Marine
Aryan

---

## ðŸ“„ License

This project is for academic use under the MIT License.

